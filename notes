important notations for machine learning:

h = hypothesis
m = number of training examples
x = input variable/features
y = output variable/target variable
(x,y) = one training example
(x (i), y (i)) = i-th training example (i is superscript)
:= assignment
= truth assertion
Î± learning rate
â€œğ››â€ means partial derivative
â€œğ“­â€ means derivative

Hypothesis:
h ğ›‰(x) = ğ›‰0 + ğ›‰1 x

Parameters:
ğ›‰0, ğ›‰1

Cost Function:
J(ğ›‰0, ğ›‰1) = 1/2m âˆ‘ (i=1 to m) (h ğ›‰(x^(i)) - y^(i))^2

Goal:
minimize J(ğ›‰0, ğ›‰1)
over ğ›‰0, ğ›‰1

The first hypothesis is given as:

h ğ›‰(x) = ğ›‰1 x
ğ›‰0 = 0
for fixed ğ›‰1, this is a function of x

Derivative: The slope of the line that is tangent to the function at certain point

ğ›‰1 := ğ›‰1 - Î±(ğ“­/ğ“­ğ›‰1)J(ğ›‰1)
where alpha is the learning rate

When the slope has a positive value:

The derivative takes a positive value
ğ›‰1 := ğ›‰1 - Î±(positive number)
We are going to decrease ğ›‰1 to get closer to the minimum.

When the slope has a negative value:

The derivate is negative
ğ›‰1 := ğ›‰1 - Î±(negative number)

When Î± is too small, gradient descent can be slow:
ğ›‰1 := ğ›‰1 - Î±(ğ“­/ğ“­ğ›‰1)J(ğ›‰1)
When Î± is small, the derivative multiplies with a small number

When Î± is too large, gradient descent can overshoot the minimum:
ğ›‰1 := ğ›‰1 - Î±(ğ“­/ğ“­ğ›‰1)J(ğ›‰1)
If ğ›‰ is close to the minimum and Î± is too big, it can fail to converge or diverge.
